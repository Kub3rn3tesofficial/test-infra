### job-env
E2E_NAME="gke-large-cluster"
PROJECT="kubernetes-scale"
# TODO: Remove FAIL_ON_GCP_RESOURCE_LEAK when PROJECT changes back to gke-large-cluster-jenkins.
FAIL_ON_GCP_RESOURCE_LEAK="false"
# TODO: should test kube-proxy test is not designed to run in large clusters.
#   We should change it start running it here too.
GINKGO_TEST_ARGS="--ginkgo.skip=\[Serial\]|\[Disruptive\]|\[Flaky\]|\[Feature:.+\]|should\stest\skube-proxy \
                         --allowed-not-ready-nodes=20 \
                         --system-pods-startup-timeout=300m"
GINKGO_PARALLEL="y"
ZONE="us-east1-a"
NUM_NODES=2000
MACHINE_TYPE="n1-standard-1"
HEAPSTER_MACHINE_TYPE="n1-standard-8"
ALLOWED_NOTREADY_NODES="20"
# We were asked (by MIG team) to not create more than 5 MIGs per zone.
# We also paged SREs with max-nodes-per-pool=400 (5 concurrent MIGs)
# So setting max-nodes-per-pool=1000, to check if that helps.
GKE_CREATE_FLAGS="--max-nodes-per-pool=1000"
CLOUDSDK_CONTAINER_USE_CLIENT_CERTIFICATE=True
CLOUDSDK_API_ENDPOINT_OVERRIDES_CONTAINER="https://staging-container.sandbox.googleapis.com/"
KUBE_GKE_IMAGE_TYPE="gci"

